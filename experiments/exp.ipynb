{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7793d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Rag-based-cover-letter---cold-email-builder\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader,TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcd1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_pdf_or_text_files(resume_pdf_path,job_dis_text):\n",
    "#     all_docs=[]\n",
    "#     resume_pdf_path=Path(resume_pdf_path)\n",
    "#     job_dis_path=\"D:\\Projects\\Rag-based-cover-letter---cold-email-builder\\data\\job_discription.txt\"\n",
    "\n",
    "#     resume_pdf_files=list(resume_pdf_path.glob(\"**/*.pdf\"))\n",
    "\n",
    "#     print(f\"Found {len(resume_pdf_files)} PDF files to process\")\n",
    "\n",
    "#     for pdf in resume_pdf_files:\n",
    "#         try:\n",
    "#             loader=PyPDFLoader(str(pdf))\n",
    "#             docs=loader.load()\n",
    "\n",
    "#             with open(job_dis_path ,\"w\") as file:\n",
    "#                 file.write(job_dis_text)\n",
    "\n",
    "#             text_loader=TextLoader(Path(job_dis_path))\n",
    "#             texts=text_loader.load()\n",
    "\n",
    "#             all_docs.extend(docs)\n",
    "#             all_docs.extend(texts)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"got an error--> {e}\")\n",
    "\n",
    "#     return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32253052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs=process_pdf_or_text_files(resume_pdf_path=\"D:\\Projects\\Rag-based-cover-letter---cold-email-builder\\data\",job_dis_text=\"\"\"Job Description\n",
    "# You’re not the person who will settle for just any role. Neither are we. Because we’re out to create Better Care for a Better World, and that takes a certain kind of person and teams who care about making a difference. Here, you’ll bring your professional expertise, talent, and drive to building and managing our portfolio of iconic, ground-breaking brands. In this role, you’ll help us deliver better care for billions of people around the world. It starts with YOU.\n",
    "\n",
    "# In This Role, You Will\n",
    "\n",
    "# Play a pivotal role in designing and building analytics solutions to facilitate informed decision-making. You will work closely with various R&D and DTS technology teams to design and implement scalable data pipelines, design analytics within R&D solutions, and ensure the accuracy and availability of data for analytics and reporting. Primary focus of the position is to design, develop, and maintain analytics solutions. Kimberly-Clark is seeking a motivated and skilled data scientist to join our dynamic team. Customers: Research & Development, Global Growth, and Quality Assurance.\n",
    "\n",
    "# Collaborate with engineering and architecture teams to identify, collect, and harmonize data from various sources.\n",
    "# Design and develop ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) pipelines to process and curate data sets using technologies such as SQL Server, Azure Data Factory and Databricks.\n",
    "# Develop and maintain data models and data warehouses using platforms like SQL Server, Azure Data Factory, Snowflake, and Databricks.\n",
    "# Apply metadata-driven frameworks to ensure scalable data ingestion and processing.\n",
    "# Implement data quality checks and validation frameworks to maintain high data standards.\n",
    "# Build and maintain data development standards and principles, providing guidance and project-specific recommendations.\n",
    "# Build models that are interpretable, scalable, and meet business needs.\n",
    "# Develop visualizations to demonstrate the results of data models to stakeholders and leadership, leveraging Microsoft Azure technologies.\n",
    "# Test and validate analytics solutions to ensure data integrity and actual results meet expected results.\n",
    "# Work with principal architect, product owners, solution engineers, business customers, and other key stakeholders to translate requirements into technical designs.\n",
    "# Mentor junior engineers and team members on data engineering techniques and best practices.\n",
    "# Train and build the talent of business users to maximize the return on investment of the analytics solutions.\n",
    "# Use Agile methodologies and tools to deliver products in a fast-paced environment.\n",
    "# Collaborate with platform teams to design and build automated processes for pipeline construction, testing, and code migration.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca66b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25de86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_chunks(docs, chunk_size=1000, chunk_overlap=200):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         length_function=len,\n",
    "#         is_separator_regex=False,\n",
    "#         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "#     )\n",
    "\n",
    "#     # Accept either a list of LangChain Document objects or list-of-strings.\n",
    "#     if not docs:\n",
    "#         return []\n",
    "\n",
    "#     try:\n",
    "#         if hasattr(docs[0], \"page_content\"):\n",
    "#             # preserve metadata when splitting Document objects\n",
    "#             doc_chunks = text_splitter.split_documents(docs)\n",
    "#         else:\n",
    "#             # ensure we pass a list of strings to create_documents\n",
    "#             texts = [str(d) for d in docs]\n",
    "#             doc_chunks = text_splitter.create_documents(texts)\n",
    "#     except Exception:\n",
    "#         # As a final fallback, convert everything to strings and split.\n",
    "#         texts = [d.page_content if hasattr(d, \"page_content\") else str(d) for d in docs]\n",
    "#         doc_chunks = text_splitter.create_documents(texts)\n",
    "\n",
    "#     return doc_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b274c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_chunks=generate_chunks(docs=docs)\n",
    "# doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90f3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd9ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embeddingsGenerator:\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def generate_embeddings(self, doc_chunks: List[str]) -> np.ndarray:\n",
    "        texts = [doc.page_content if hasattr(doc, \"page_content\") else str(doc) for doc in doc_chunks]\n",
    "        embeddings=self.model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4dfaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_embeddings(doc_chunks):\n",
    "#     embedder=SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "#     # Extract page_content from Document objects\n",
    "#     texts = [doc.page_content if hasattr(doc, \"page_content\") else str(doc) for doc in doc_chunks]\n",
    "#     embeddings=embedder.encode(texts, show_progress_bar=True)\n",
    "#     return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375f3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings=embeddingsGenerator().generate_embeddings(doc_chunks=doc_chunks)\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc112cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vectorStore:\n",
    "    def __init__(self, collection_name: str = \"resume_job_descriptions\", dir:str=None):\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            persist_directory=dir,\n",
    "        ))\n",
    "        self.collection = self.client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    def add_embeddings(self, doc_chunks: List[Any], embeddings: np.ndarray):\n",
    "        # Validate lengths\n",
    "        if doc_chunks is None or embeddings is None:\n",
    "            raise ValueError(\"doc_chunks and embeddings must be provided\")\n",
    "\n",
    "        if len(doc_chunks) != len(embeddings):\n",
    "            print(f\"Length of docs not equal to length of embeddings,lengths are {len(doc_chunks)} and {len(embeddings)} respectively\")\n",
    "            return\n",
    "\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for doc, emb in zip(doc_chunks, embeddings):\n",
    "            # Support both Document objects and plain strings\n",
    "            text = doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
    "            documents_text.append(text)\n",
    "\n",
    "            # Ensure embedding is a plain list of floats\n",
    "            if hasattr(emb, \"tolist\"):\n",
    "                embeddings_list.append(emb.tolist())\n",
    "            else:\n",
    "                embeddings_list.append(list(emb))\n",
    "\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(documents_text))]\n",
    "        self.collection.add(\n",
    "            documents=documents_text,\n",
    "            embeddings=embeddings_list,\n",
    "            ids=ids\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40e2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store=vectorStore(dir=\"D:/Projects/Rag-based-cover-letter---cold-email-builder/vectorstore\")\n",
    "# vector_store.add_embeddings(doc_chunks=doc_chunks, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33968da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ragpipeline:\n",
    "    def __init__(self, vector_store: vectorStore, embeddings_generator: embeddingsGenerator):\n",
    "        self.vector_store = vector_store\n",
    "        self.embeddings_generator = embeddings_generator\n",
    "\n",
    "    def query(self, query_text: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        query_embedding = self.embeddings_generator.model.encode([query_text])[0]\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        retrieved_docs = results['documents'][0]\n",
    "        scores = results['distances'][0]\n",
    "        \n",
    "        return list(zip(retrieved_docs, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb93a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_pipeline = Ragpipeline(vector_store, embeddingsGenerator())\n",
    "# response=response_pipeline.query(query_text=\"Generate a short and crisp cover letter position\", top_k=3)\n",
    "# response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0760ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7648d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca563e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMResponseGenerator:\n",
    "    def __init__(self, resume_pdf_path):\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.resume_pdf_path = resume_pdf_path\n",
    "\n",
    "    def process_pdf_or_text_files(self):\n",
    "        all_docs=[]\n",
    "        resume_pdf_path=Path(self.resume_pdf_path)\n",
    "\n",
    "        resume_pdf_files=list(resume_pdf_path.glob(\"**/*.pdf\"))\n",
    "\n",
    "        print(f\"Found {len(resume_pdf_files)} PDF files to process\")\n",
    "\n",
    "        for pdf in resume_pdf_files:\n",
    "            try:\n",
    "                loader=PyPDFLoader(str(pdf))\n",
    "                docs=loader.load()\n",
    "                all_docs.extend(docs)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"got an error--> {e}\")\n",
    "\n",
    "        return all_docs\n",
    "    \n",
    "    def generate_chunks(self, docs, chunk_size=1000, chunk_overlap=200):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "\n",
    "        if not docs:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            if hasattr(docs[0], \"page_content\"):\n",
    "                doc_chunks = text_splitter.split_documents(docs)\n",
    "            else:\n",
    "                texts = [str(d) for d in docs]\n",
    "                doc_chunks = text_splitter.create_documents(texts)\n",
    "        except Exception:\n",
    "            texts = [d.page_content if hasattr(d, \"page_content\") else str(d) for d in docs]\n",
    "            doc_chunks = text_splitter.create_documents(texts)\n",
    "\n",
    "        return doc_chunks\n",
    "\n",
    "    def generate_response(self, query: str) -> str:\n",
    "        # Gather docs and create chunks\n",
    "        all_docs = self.process_pdf_or_text_files()\n",
    "        doc_chunks = self.generate_chunks(all_docs)\n",
    "\n",
    "        # Generate embeddings for chunks\n",
    "        embedder = embeddingsGenerator()\n",
    "        embeddings = embedder.generate_embeddings(doc_chunks)\n",
    "\n",
    "        # Persist embeddings to vector store\n",
    "        vector_store = vectorStore(dir=\"D:/Projects/Rag-based-cover-letter---cold-email-builder/vectorstore\")\n",
    "        vector_store.add_embeddings(doc_chunks=doc_chunks, embeddings=embeddings)\n",
    "\n",
    "        # Build a retrieval pipeline and query it\n",
    "        response_pipeline = Ragpipeline(vector_store, embedder)\n",
    "        retrieved = response_pipeline.query(query_text=query, top_k=3)\n",
    "\n",
    "        # retrieved is list of (doc_text, score); join doc_texts for context\n",
    "        context= \"\\n\\n\".join([doc for doc, _ in retrieved])\n",
    "        print(\"context--->\",context)\n",
    "\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nResponce_body:Start with introducing yourself,mention recent experience, mention highest qualification and finally mention your works and skils to prove why you are the best fit for this role.\" \n",
    "\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context---> Pratik Kujur\n",
      "6264902455 |\n",
      " pratikkujur121@gmail.com|\n",
      " PratikKujur |\n",
      " PratikKujur\n",
      "Experience\n",
      "PORTER | Machine Learning Intern Jan 2025 – Present\n",
      "Skills: Python, Sklearn, Snowflake, Mlflow, Chalk AI, GCP, BigQuery, Streamlit Bangalore, Karnataka\n",
      "• Developed and deployed production-grade ML models (Logistic Regression, XGBoost) on GCP , achieving\n",
      "Precision: 0.79 and Recall: 0.76 with 100K+ daily predictions and ensure 99.9% uptime.\n",
      "• Orchestrated a cross-platform ETL pipeline using Airflow DAG integrating BigQuery and Snowflake for\n",
      "feature retrieval, processing 5M+ records daily for model training .\n",
      "• Engineered 15+ features using Chalk AI (Feature Store) to strengthen training pipelines for Driver Ranking\n",
      "models and monitor city-wise model performance.\n",
      "• Automated real-time feature skewness monitoring to ensure data stability and feature correctness.\n",
      "• Contributed to MLOPS dashboard to observe Concept drift and Feature drift in Driver-Ranking models.\n",
      "Education\n",
      "\n",
      "Pratik Kujur\n",
      "6264902455 |\n",
      " pratikkujur121@gmail.com|\n",
      " PratikKujur |\n",
      " PratikKujur\n",
      "Experience\n",
      "PORTER | Machine Learning Intern Jan 2025 – Present\n",
      "Skills: Python, Sklearn, Snowflake, Mlflow, Chalk AI, GCP, BigQuery, Streamlit Bangalore, Karnataka\n",
      "• Developed and deployed production-grade ML models (Logistic Regression, XGBoost) on GCP , achieving\n",
      "Precision: 0.79 and Recall: 0.76 with 100K+ daily predictions and ensure 99.9% uptime.\n",
      "• Orchestrated a cross-platform ETL pipeline using Airflow DAG integrating BigQuery and Snowflake for\n",
      "feature retrieval, processing 5M+ records daily for model training .\n",
      "• Engineered 15+ features using Chalk AI (Feature Store) to strengthen training pipelines for Driver Ranking\n",
      "models and monitor city-wise model performance.\n",
      "• Automated real-time feature skewness monitoring to ensure data stability and feature correctness.\n",
      "• Contributed to MLOPS dashboard to observe Concept drift and Feature drift in Driver-Ranking models.\n",
      "Education\n",
      "\n",
      "engagement and enabling fully automated, real-time wellness recommendations.\n",
      "Melanoma Skin Cancer Detection Using VGG16 |\n",
      " Github|\n",
      " Project Link\n",
      "Skills: Python,TensorFlow, Keras, MLflow, DVC, AWS (S3, EC2), Flask\n",
      "• Developed end-to-end deep learning system using transfer learning with VGG16 architecture, achieving 87%\n",
      "accuracy in binary classification of 10,000+ dermatoscopic images into benign and melanoma categories\n",
      "• Implemented comprehensive MLOps workflow with MLflow for experiment tracking (15+ experiments) and DVC\n",
      "for dataset versioning, ensuring reproducibility and enabling rollback capabilities for model iterations\n",
      "NLP-Based Hate Sentiment Classifier. |\n",
      " Github|\n",
      " Project Link\n",
      "Skills:Python, TensorFlow, Keras, NLTK, FastAPI\n",
      "• Architected LSTM-based sentiment analysis model for binary classification of 50K+ tweets, achieving 85%\n",
      "accuracy and F1-score of 0.83.\n",
      "• Preprocessed textual data using advanced NLP techniques including tokenization, lemmatization, and custom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text=\"\"\"Dear Hiring Manager,\n",
       "\n",
       "I am writing to express my enthusiastic interest in the Data Scientist position at Kimberly-Clark, as advertised. With a strong commitment to leveraging data for impactful solutions, I am particularly drawn to your mission of \"Better Care for a Better World\" and believe my skills align perfectly with your team's objectives.\n",
       "\n",
       "Currently serving as a Machine Learning Intern at PORTER, I have been instrumental in designing and deploying production-grade ML models and robust data pipelines. My experience includes orchestrating cross-platform ETL pipelines using Airflow, integrating BigQuery and Snowflake to process millions of records daily for model training. This directly correlates with your need for developing scalable data pipelines and managing data models/warehouses.\n",
       "\n",
       "While my full educational background is not included here, my practical experience demonstrates a strong foundation in data science and machine learning principles. I have hands-on expertise in developing interpretable models (Logistic Regression, XGBoost, LSTM) on cloud platforms like GCP, ensuring high uptime and performance for 100K+ daily predictions. Furthermore, my work involves engineering features using tools like Chalk AI, implementing real-time feature skewness monitoring, and contributing to MLOps dashboards for concept and feature drift, directly addressing your requirements for data quality, accuracy, and robust analytics solutions within R&D.\n",
       "\n",
       "My project experience, including developing an end-to-end deep learning system for Melanoma Detection and an NLP-based Hate Sentiment Classifier, further highlights my proficiency in MLOps workflows using MLflow and DVC, and my ability to build effective, scalable solutions from diverse datasets. I am confident in my ability to collaborate with R&D and DTS technology teams to build analytics solutions, develop insightful visualizations, and mentor junior team members, as well as adapt quickly to new technologies such as Azure Data Factory and Databricks given my existing cloud experience.\n",
       "\n",
       "I am eager to contribute my expertise in building and maintaining cutting-edge analytics solutions to drive informed decision-making at Kimberly-Clark. Thank you for considering my application.\n",
       "\n",
       "Sincerely,\n",
       "Pratik Kujur\n",
       "6264902455\n",
       "pratikkujur121@gmail.com\"\"\"\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "      index=0\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='sKU6aa3hHOSmg8UPqczr2QM',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=445,\n",
       "    prompt_token_count=1299,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=1299\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=2228,\n",
       "    total_token_count=3972\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cover_letter=LLMResponseGenerator(resume_pdf_path=\"D:\\Projects\\Rag-based-cover-letter---cold-email-builder\\data\")\n",
    "get_cover_letter.generate_response(query=\"\"\"Generate a short and crisp cover letter for this following job description-->Job Description\n",
    "\n",
    "You’re not the person who will settle for just any role. Neither are we. Because we’re out to create Better Care for a Better World, and that takes a certain kind of person and teams who care about making a difference. Here, you’ll bring your professional expertise, talent, and drive to building and managing our portfolio of iconic, ground-breaking brands. In this role, you’ll help us deliver better care for billions of people around the world. It starts with YOU.\n",
    "\n",
    "In This Role, You Will\n",
    "\n",
    "Play a pivotal role in designing and building analytics solutions to facilitate informed decision-making. You will work closely with various R&D and DTS technology teams to design and implement scalable data pipelines, design analytics within R&D solutions, and ensure the accuracy and availability of data for analytics and reporting. Primary focus of the position is to design, develop, and maintain analytics solutions. Kimberly-Clark is seeking a motivated and skilled data scientist to join our dynamic team. Customers: Research & Development, Global Growth, and Quality Assurance.\n",
    "\n",
    "Collaborate with engineering and architecture teams to identify, collect, and harmonize data from various sources.\n",
    "Design and develop ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) pipelines to process and curate data sets using technologies such as SQL Server, Azure Data Factory and Databricks.\n",
    "Develop and maintain data models and data warehouses using platforms like SQL Server, Azure Data Factory, Snowflake, and Databricks.\n",
    "Apply metadata-driven frameworks to ensure scalable data ingestion and processing.\n",
    "Implement data quality checks and validation frameworks to maintain high data standards.\n",
    "Build and maintain data development standards and principles, providing guidance and project-specific recommendations.\n",
    "Build models that are interpretable, scalable, and meet business needs.\n",
    "Develop visualizations to demonstrate the results of data models to stakeholders and leadership, leveraging Microsoft Azure technologies.\n",
    "Test and validate analytics solutions to ensure data integrity and actual results meet expected results.\n",
    "Work with principal architect, product owners, solution engineers, business customers, and other key stakeholders to translate requirements into technical designs.\n",
    "Mentor junior engineers and team members on data engineering techniques and best practices.\n",
    "Train and build the talent of business users to maximize the return on investment of the analytics solutions.\n",
    "Use Agile methodologies and tools to deliver products in a fast-paced environment.\n",
    "Collaborate with platform teams to design and build automated processes for pipeline construction, testing, and code migration.\"\"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e36435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag-based-cover-letter---cold-email-builder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
